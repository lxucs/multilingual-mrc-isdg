basic {
  data_dir = /home/ubuntu/clqa_data_dir  # Edit this
  download_dir = ${basic.data_dir}/download  # dir that contains downloaded dataset
  log_root = ${basic.data_dir}
}

#*************** Dataset-specific config ***************
dataset = ${basic} {
  max_segment_len = 384
  doc_stride = 128
  max_query_len = 64
  max_answer_len = 30
  n_best_predictions = 10
  version_2_with_negative = False  # Only true for squad 2
  null_score_diff_threshold = 0.0
}

#*************** Model-specific config ***************

model {
  # Learning
  num_epochs = 2
  batch_size = 16
  bert_learning_rate = 3e-5
  # task_learning_rate = 2e-4
  adam_eps = 1e-8
  adam_weight_decay = 1e-4
  warmup_ratio = 0.1
  max_grad_norm = 1
  gradient_accumulation_steps = 1

  # Other
  eval_frequency = 1000
  report_frequency = 100

  train_dataset = squad
}

mbert_base = ${model}{
  model_type = bert
  pretrained = bert-base-multilingual-cased
}

xlmr_base = ${model}{
  model_type = xlm-roberta
  pretrained = xlm-roberta-base
}

xlmr_large = ${model}{
  model_type = xlm-roberta
  pretrained = xlm-roberta-large
}

mt5_large = ${model} {
  model_type = mt5
  pretrained = google/mt5-large
  bert_learning_rate = 1e-4
  batch_size = 8
  gradient_accumulation_steps = 2
}

#*************** Experiment-specific config ***************

mbert_base_zero_shot = ${dataset} ${mbert_base} {
  zero_shot = true
}

xlmr_base_zero_shot = ${dataset} ${xlmr_base} {
  zero_shot = true
}

xlmr_large_zero_shot = ${dataset} ${xlmr_large} {
  zero_shot = true
}

mt5_large_zero_shot = ${dataset} ${mt5_large} {
  zero_shot = true
}

mbert_base_zero_shot_tydiqa = ${mbert_base_zero_shot} {
  train_dataset = tydiqa
  num_epochs = 4
}

xlmr_base_zero_shot_tydiqa = ${xlmr_base_zero_shot} {
  train_dataset = tydiqa
  num_epochs = 4
}

xlmr_large_zero_shot_tydiqa = ${xlmr_large_zero_shot} {
  train_dataset = tydiqa
  num_epochs = 4
}

mt5_large_zero_shot_tydiqa = ${mt5_large_zero_shot} {
  train_dataset = tydiqa
  num_epochs = 4
}
